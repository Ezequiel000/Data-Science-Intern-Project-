{"cells":[{"cell_type":"markdown","metadata":{"id":"k64Z8hXNy-sh"},"source":["#Modules"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30617,"status":"ok","timestamp":1660780407001,"user":{"displayName":"Ezequiel Hernandez","userId":"10229188064205579760"},"user_tz":420},"id":"I_iftfZn2Ndo","outputId":"d5a6ab91-0afc-4485-a744-db458165952b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import os\n","import torch \n","import numpy as np\n","import pandas as pd\n","import h5py\n","import math\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torchvision import datasets, transforms\n","from torch.cuda.amp import GradScaler\n","from sklearn.metrics import f1_score\n","from google.colab import drive\n","from collections import deque\n","drive.mount('/content/drive')\n","FMRI_DIR_STND = '/content/drive/Shareddrives/Summer Intership 2022 - Brain Dataset/data/standardized_betas/subj01'\n","PATH = '/content/drive/Shareddrives/Summer Intership 2022 - Brain Dataset/data/standardized_betas/checkpoint' #path for checkpoint"]},{"cell_type":"markdown","metadata":{"id":"sCXh69nsJGLM"},"source":["#Debugger"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":896},"executionInfo":{"elapsed":8222,"status":"ok","timestamp":1660187525256,"user":{"displayName":"Ezequiel Hernandez","userId":"10229188064205579760"},"user_tz":420},"id":"BqqKMVOIMqA6","outputId":"ac3e83d1-a9bc-40cd-8831-bdc6df505a3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ipdb\n","  Downloading ipdb-0.13.9.tar.gz (16 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.4.0)\n","Collecting ipython>=7.17.0\n","  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n","\u001b[K     |████████████████████████████████| 793 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.1.3)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.18.1)\n","Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n","  Downloading prompt_toolkit-3.0.30-py3-none-any.whl (381 kB)\n","\u001b[K     |████████████████████████████████| 381 kB 49.8 MB/s \n","\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.1.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n","Building wheels for collected packages: ipdb\n","  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ipdb: filename=ipdb-0.13.9-py3-none-any.whl size=11648 sha256=12bb7a656fc1d65e5ca83b25f2213a92312b63ee95c3f5180bb2d904a8c0ce6c\n","  Stored in directory: /root/.cache/pip/wheels/65/cd/cc/aaf92acae337a28fdd2aa4d632196a59745c8c39f76eaeed01\n","Successfully built ipdb\n","Installing collected packages: prompt-toolkit, ipython, ipdb\n","  Attempting uninstall: prompt-toolkit\n","    Found existing installation: prompt-toolkit 1.0.18\n","    Uninstalling prompt-toolkit-1.0.18:\n","      Successfully uninstalled prompt-toolkit-1.0.18\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 5.5.0\n","    Uninstalling ipython-5.5.0:\n","      Successfully uninstalled ipython-5.5.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.30 which is incompatible.\n","google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.34.0 which is incompatible.\u001b[0m\n","Successfully installed ipdb-0.13.9 ipython-7.34.0 prompt-toolkit-3.0.30\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","prompt_toolkit"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip install ipdb\n","import ipdb"]},{"cell_type":"markdown","metadata":{"id":"gurPuhWfJLPG"},"source":["#Dataset Class"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":329,"status":"ok","timestamp":1660780426301,"user":{"displayName":"Ezequiel Hernandez","userId":"10229188064205579760"},"user_tz":420},"id":"91i9gOEC6bow"},"outputs":[],"source":["class DatasetNSD(Dataset):\n","  def __init__(self):\n","      #fmri_files will store all the betas files in the given directory\n","      self.dir = FMRI_DIR_STND\n","      self.fmri_files = self.getDirFiles()\n","      # num_of_scans is set by getRanges\n","      self.num_of_scans = 0\n","      # index_ranges is a dictionery with index range as key and corresponding file as value\n","      self.index_ranges =  self.getRanges()\n","      self.file_handlers = {}\n","      self.open_files()\n","\n","  def __del__(self):\n","    for key, value in self.file_handlers.items():\n","      value.close()\n","\n","\n","  def __len__(self):\n","    return self.num_of_scans \n","\n","#Input: hdf5 file name \n","#Output: store hdf5 in deque, return file object\n","  def open_files(self): \n","    for file_name in self.fmri_files:\n","      path = os.path.join(FMRI_DIR_STND, file_name)\n","      self.file_handlers[file_name] = h5py.File(path, 'r')\n","\n","   \n","  def __getitem__(self, index):\n","    #index will be 0-17908\n","    # files come in diffirent sizes of fmri scans.\n","    # a file will be chose based on the range of the index \n","    for key in self.index_ranges:\n","      index_range = list(key)\n","      if (index in index_range):\n","        f = self.file_handlers[self.index_ranges[key]]\n","        indx = index - f['startIndx/i'][0]\n","        sample = torch.FloatTensor(np.array(f['betas/b'][indx]))\n","        label = torch.FloatTensor(np.array(f['labels/l'][indx]))\n","        break\n","    return (sample, label)\n","\n","     \n","  def getDirFiles(self):\n","     files = [f for f in os.listdir(self.dir) if \n","              os.path.isfile(os.path.join(self.dir, f)) and\n","              f[-5:] == '.hdf5']\n","\n","     files.sort()                          \n","     return files\n","\n","  def getRanges(self):\n","    ranges = {}\n","    previous = 0\n","    count = 0\n","    for x in self.fmri_files:\n","      with h5py.File(os.path.join(self.dir, x), \"r\") as f:    \n","          size = f['labels/l'].shape[0]\n","          count = count + size\n","          if previous == 0:\n","            ranges[range(size)] = x\n","            previous = size\n","          else:\n","            size = size + previous\n","            i = range(previous,size)\n","            ranges[i] = x\n","            previous = size\n","    self.num_of_scans = count\n","    return ranges"]},{"cell_type":"markdown","metadata":{"id":"5bTp53LFIqZf"},"source":["#NeuralNet3"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1660780428850,"user":{"displayName":"Ezequiel Hernandez","userId":"10229188064205579760"},"user_tz":420},"id":"J8lB0a33Is9G"},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.kernel = 2\n","        self.stride = 2 \n","        self.conv_stack = nn.Sequential(\n","        nn.Conv3d(1,out_channels=64 , kernel_size = 3, stride = 3),\n","        nn.LeakyReLU(),\n","        nn.Conv3d(in_channels=64,out_channels=32 , kernel_size = self.kernel, stride = self.stride),\n","        nn.LeakyReLU(),\n","        nn.Conv3d(in_channels=32,out_channels=16 , kernel_size = self.kernel, stride = self.stride),\n","        nn.LeakyReLU(),\n","        nn.Flatten()\n","        )\n","        self.linear_stack = nn.Sequential(\n","        nn.Linear(4608,2000),\n","        nn.ReLU(),\n","        nn.Dropout(p=0.2),\n","        nn.Linear(2000,1000),\n","        nn.ReLU(),\n","        nn.Dropout(p=0.2),\n","        nn.Linear(1000,500),\n","        nn.ReLU(),\n","        nn.Dropout(p=0.2),\n","        nn.Linear(500,256),\n","        nn.ReLU(),\n","        nn.Linear(256,128),\n","        nn.ReLU(),\n","        nn.Linear(128,64),\n","        nn.ReLU(),\n","        nn.Linear(64, 1),\n","          )\n","\n","    # # [(input_volume−kernel_size)/stride]+1\n","    # def output_size(self,volume):\n","    #   size = ((volume -self.kernel)//self.stride) + 1\n","    #   return size\n","\n","    def forward(self, x):\n","        x = self.conv_stack(x.unsqueeze(dim=1))\n","        x = self.linear_stack(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"rYzRjZlLny_5"},"source":["#DataLoaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DNnQPJOdlXtA"},"outputs":[],"source":["BATCH_SIZE = 199\n","ds = DatasetNSD()\n","split = len(ds)//6\n","split = len(ds)-split\n","#split =14925\n","train_dataset = Subset(ds, range(len(ds))[: split] )\n","test_dataset = Subset(ds, range(len(ds))[split: ])\n","train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"Q18L9Sj4I9Lf"},"source":["#Initialize NeuralNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MtlyaMcFnsgI"},"outputs":[],"source":["#device config\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")\n","\n","#hyper parameter\n","EPOCHS = 20\n","LEARNING_RATE = 0.0001\n","WEIGHT_DECAY = 0.01\n","\n","modelNN = CNN().to(device)\n","# modelNN.load_state_dict(torch.load(os.path.join(PATH,'model_weights.pth')))\n","# weight = torch.FloatTensor([.322]).to(device)\n","loss_func = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(modelNN.parameters(), lr = LEARNING_RATE)\n","\n","print(modelNN)"]},{"cell_type":"markdown","metadata":{"id":"FGLKWaKvbebA"},"source":["#Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6KqiBL1e2SwL"},"outputs":[],"source":["from tqdm import tqdm\n","def trainingLoop(dataloader, model, loss_func, optimizer , test_loader):\n","  n_total_steps = len(dataloader)\n","  scaler = GradScaler()\n","  trues = []\n","  preds = []\n","  for epoch in range(EPOCHS):\n","    model.train()\n","    for i, (sample, true_label) in enumerate(train_loader):\n","      sample = sample.to(device)\n","      trues.extend(true_label.numpy())\n","      true_label = true_label.to(device).unsqueeze(dim=1)\n","      pred_label = model(sample)\n","      loss = loss_func(pred_label, true_label)\n","      train_loss =  loss.item()\n","      #backward pass\n","      optimizer.zero_grad()\n","      scaler.scale(loss).backward()\n","      scaler.step(optimizer)\n","      scaler.update()\n","      #predictioins\n","      pred = torch.sigmoid(pred_label).detach().cpu()\n","      preds.extend(np.round(pred.reshape(-1)).numpy())\n","      if (i+1) % 2 == 0:\n","        print(f'epoch {epoch+1}/{EPOCHS},step {i+1}/{n_total_steps}, loss = {(train_loss):.4f} ')   \n","    score = f1_score(trues, preds)\n","    print(f'F1 score for training set is: {score}')\n","    torch.save(model.state_dict(), os.path.join(PATH,'model_weights.pth'))\n","    # print(f'Train loss for epoch: {train_loss/n_total_steps} ')\n","    # predTesting(test_loader, model)\n","\n","def predTesting(dataloader, model): \n","  model.eval()\n","  with torch.no_grad():\n","    n_samples = len(dataloader)\n","    # trues = []\n","    # preds = []\n","    correct = 0\n","    for  sample, true_label in tqdm(dataloader):\n","      sample = sample.to(device)\n","      # trues.extend(true_label.numpy())\n","      true_label = true_label.to(device).unsqueeze(dim=1)\n","      pred_label = model(sample)\n","      #predictioins\n","    #   pred = torch.sigmoid(pred_label).cpu()\n","    #   preds.extend(np.round(pred.reshape(-1)).numpy())\n","    # score = f1_score(trues, preds)\n","    print(f'F1 score for training set is: {score}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A8synMMwQ4t2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8df6a1f4-7e61-49d5-dd27-40ed4549c3d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1/20,step 2/75, loss = 0.7141 \n","epoch 1/20,step 4/75, loss = 0.7079 \n","epoch 1/20,step 6/75, loss = 0.7069 \n","epoch 1/20,step 8/75, loss = 0.7069 \n","epoch 1/20,step 10/75, loss = 0.7031 \n","epoch 1/20,step 12/75, loss = 0.6957 \n","epoch 1/20,step 14/75, loss = 0.6861 \n","epoch 1/20,step 16/75, loss = 0.6732 \n","epoch 1/20,step 18/75, loss = 0.6410 \n","epoch 1/20,step 20/75, loss = 0.6073 \n","epoch 1/20,step 22/75, loss = 0.5876 \n","epoch 1/20,step 24/75, loss = 0.5575 \n","epoch 1/20,step 26/75, loss = 0.5728 \n","epoch 1/20,step 28/75, loss = 0.5526 \n","epoch 1/20,step 30/75, loss = 0.5817 \n"]}],"source":["trainingLoop(train_loader, modelNN, loss_func, optimizer, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61442,"status":"ok","timestamp":1659757847479,"user":{"displayName":"Ezequiel Hernandez","userId":"10229188064205579760"},"user_tz":420},"id":"ohZn5KVajM8G","outputId":"9de3979a-c171-47c0-d6b1-8acb71e396cd"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 150/150 [01:01<00:00,  2.45it/s]"]},{"name":"stdout","output_type":"stream","text":["F1 score for training set is: 0.8692686623721106\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["predTesting(test_loader, modelNN, loss_func)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"NeuralNet.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}